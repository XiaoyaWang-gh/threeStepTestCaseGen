import path
import torch as t
from transformers import AutoConfig, AutoTokenizer
from plbartCopyGenerator import PLBartCopyGenerator

#æ¯ä¸ªdefects4jä¸­çš„é¡¹ç›®ä½œä¸ºä¸€ä¸ª.txtæ–‡ä»¶
file_path = path.pro_lang

#åŠ è½½æ¨¡å‹
trained_path = path.my_ckpt_epoch_6
config = AutoConfig.from_pretrained(trained_path)
model = PLBartCopyGenerator.from_pretrained(trained_path,config=config)
#åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(path.hf_checkpoint)


# è¯»å–d4jä¸­çš„focal methods,keyæ˜¯é¡¹ç›®åç§°+ç¼ºé™·åºå·
def read_file()->dict():
    langDict = dict()
    with open(file_path,"r") as file:
        for line in file:
            key, value = line.split(',',1)
            langDict[key] = value
    return langDict

def cut_seq(input) -> str:
    if len(input) > 1024:
        input = input[:1024]
    return input

def test_model(input): # ä¸ç”¨Generate
    tokenized_input = tokenizer(input,add_special_tokens=False,return_tensors="pt")
    input_ids = tokenized_input.input_ids
    seq2seqLMOutput = model(input_ids)
    probs = t.softmax(seq2seqLMOutput.logits, dim=-1)
    output_tokens = probs.argmax(dim=-1)
    outputs = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)

    print(outputs[0])

def test_model_gen(input): # ä½¿ç”¨Generate()
    tokenized_input = tokenizer(input,add_special_tokens=False,return_tensors="pt")
    input_ids = tokenized_input.input_ids
    # å¯¹ç”Ÿæˆåšè¦æ±‚
    gen_kwargs = {
        "no_repeat_ngram_size":3,
        "max_length":200,
        "min_length":40,
    }
    outputs = model.generate(inputs=input_ids,**gen_kwargs)
    output_tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    print(output_tokens[0])
    pass
 

def main():
    langDict = read_file()
    for k,v in langDict.items():
        print(k,"ğŸ˜ƒ")
        print("focal method ğŸ‘‡\n",v)
        print("focal method ğŸ‘†")
        print("test prefix ğŸ‘‡")
        v = cut_seq(v)
        test_model_gen(v)
        print("test prefix ğŸ‘† ")


if __name__ == "__main__":
    main()
